{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e93bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from generative.inferers import DiffusionInferer\n",
    "from generative.networks.nets import DiffusionModelUNet\n",
    "from generative.networks.schedulers import DDPMScheduler\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from monai.utils import first, set_determinism\n",
    "from monai.transforms import Compose, LoadImage, ToTensor, ScaleIntensity, CenterSpatialCrop, Resize, EnsureChannelFirst, RandAffined, SaveImage, Rotate90\n",
    "from monai.data import CacheDataset, DataLoader, Dataset, ArrayDataset\n",
    "import os\n",
    "from torch.utils.data import ConcatDataset,random_split\n",
    "from monai.config import print_config\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0032b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_determinism(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcc485c",
   "metadata": {},
   "source": [
    "### Loading trained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafdb747",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DiffusionModelUNet(\n",
    "    spatial_dims=2,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    num_channels=(128, 256, 256), #256, 256, 512\n",
    "    attention_levels=(False, True, True),\n",
    "    num_res_blocks=1,\n",
    "    num_head_channels=256,\n",
    ")\n",
    "device = torch.device(\"cuda\")\n",
    "#modelname = \"Models/bs16_Epoch124_of_2503nov\" #74 / 124 / 174\n",
    "#modelname = \"Models/bs16_Epoch124_of_2503nov\"\n",
    "#modelname = \"Models/bs16_Epoch149_of_2008nov_timestep500\"\n",
    "#modelname = \"Models/bs8_Epoch149_of_2008nov\"\n",
    "modelname = \"Models/bs16_Epoch149_of_2503nov\"\n",
    "pre_trained_model = torch.load(modelname) #,map_location=torch.device('cpu'))\n",
    "model.load_state_dict(pre_trained_model, strict = False) \n",
    "model.to(device)\n",
    "\n",
    "\n",
    "scheduler = DDPMScheduler(num_train_timesteps=1000)#1000\n",
    "inferer = DiffusionInferer(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfebf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970fa7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "noise = torch.randn((1, 1, 128, 128))\n",
    "noise = noise.to(device)\n",
    "scheduler.set_timesteps(num_inference_steps=1000)\n",
    "with autocast(enabled=True):\n",
    "    image, intermediates = inferer.sample(\n",
    "        input_noise=noise, diffusion_model=model, scheduler=scheduler, save_intermediates=True, intermediate_steps=200\n",
    "    )\n",
    "\n",
    "chain = torch.cat(intermediates, dim=-1)\n",
    "\n",
    "plt.style.use(\"default\")\n",
    "plt.imshow(chain[0, 0].cpu(), vmin=0, vmax=1, cmap=\"gray\")\n",
    "plt.tight_layout()\n",
    "plt.axis(\"off\")\n",
    "plt.figure(figsize=(30,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d65a73",
   "metadata": {},
   "source": [
    "### Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f08c4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Compose(\n",
    "    [LoadImage(image_only = True),\n",
    "     EnsureChannelFirst(),\n",
    "     ToTensor(),\n",
    "     #ScaleIntensity(minv = 0.0, maxv = 1.0),\n",
    "     #CenterSpatialCrop(roi_size=(256,256,-1)),\n",
    "     Resize(spatial_size=(128, 128, -1)),\n",
    "     #ScaleIntensity(minv = 0.0, maxv = 1.0),\n",
    "     #Rotate90(k=3, spatial_axes=(0, 1), lazy=False)\n",
    "     ])\n",
    "\n",
    "class NiFTIDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform = None):\n",
    "        self.data_dir = data_dir\n",
    "        self.data = os.listdir(data_dir)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        nifti_file = os.path.join(self.data_dir, self.data[index])\n",
    "        if self.transform is not None:\n",
    "            nifti_file = self.transform(nifti_file)\n",
    "        return nifti_file\n",
    "\n",
    "image_transforms = Compose(\n",
    "    [\n",
    "     #Resize(spatial_size=(128, 128,-1)),\n",
    "     ScaleIntensity(minv = 0.0, maxv = 1.0),\n",
    "     Rotate90(k=3, spatial_axes=(0, 1), lazy=False),\n",
    "     ])\n",
    "\n",
    "def extract_slices(nifti_dataset): \n",
    "    total_dataset = Dataset([])\n",
    "    for i in range(len(nifti_dataset)):#Skrev på Dataset om nifti_dataset\n",
    "        image_stack = Dataset(nifti_dataset).__getitem__(index = i)\n",
    "        for j in range(image_stack.shape[3]):\n",
    "            image_stack[:,:,:,j] = image_transforms(image_stack[:,:,:,j])\n",
    "        images = Dataset([image_stack[:,:,:,k] for k in range(3, image_stack.shape[3] - 3)])\n",
    "        total_dataset = ConcatDataset([total_dataset, images])\n",
    "    \n",
    "    return total_dataset\n",
    "\n",
    "nifti_dataset = NiFTIDataset(data_dir= \"T2_images\", transform = transform)\n",
    "print(torch.amax(nifti_dataset.__getitem__(index = 0)[0,:,:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a1c5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nifti_dataset = extract_slices(nifti_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b469a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((nifti_dataset.__getitem__(0).shape))\n",
    "plt.imshow(nifti_dataset.__getitem__(0)[0], cmap = \"bone\")\n",
    "plt.colorbar()\n",
    "print(torch.amax(nifti_dataset.__getitem__(6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b842bc",
   "metadata": {},
   "source": [
    "### Saving preprocessed dataset to folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aed5035",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_images_to_save = extract_slices(nifti_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584902ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(real_images_to_save))\n",
    "print(real_images_to_save.__getitem__(0).shape)\n",
    "i = 0\n",
    "for real_image in real_images_to_save:\n",
    "    print(torch.amax(real_image[0]))\n",
    "    real_image = real_image.numpy()\n",
    "    print(np.amax(real_image[0]))\n",
    "    #nifti_image = nib.Nifti1Image(real_image[0],np.eye(4))\n",
    "    #nib.save(nifti_image, \"Real_images/Real_training_data/nifti_file_\" + str(i) + \".nii\")\n",
    "    #i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d65f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(real_images_to_save.__getitem__(0)[0], cmap = \"bone\")\n",
    "plt.colorbar()\n",
    "print(np.amax(real_images_to_save.__getitem__(0)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1a4372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hold_out(train_ratio, nifti_dataset):\n",
    "    train_patiens = int(train_ratio * len(nifti_dataset))\n",
    "    val_patiens = len(nifti_dataset) - train_patiens\n",
    "    \n",
    "    train_nifti_dataset, val_nifti_dataset = random_split(nifti_dataset, [train_patiens, val_patiens])\n",
    "    \n",
    "    train_dataset = extract_slices(train_nifti_dataset)\n",
    "    val_dataset = extract_slices(val_nifti_dataset)\n",
    "    \n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828e1b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = hold_out(0.8, nifti_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7e324a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.__getitem__(0).shape)\n",
    "print(len(val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66819cea",
   "metadata": {},
   "source": [
    "### Sampling images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6019781b",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.randn((40, 1, 128, 128)) #Hvis input noise er på formen (n_synthetic_images, 1, 64, 64) så genereres det n_synthetic images\n",
    "noise = noise.to(device)\n",
    "scheduler.set_timesteps(num_inference_steps=1000)\n",
    "\n",
    "images = inferer.sample(input_noise=noise, diffusion_model=model, scheduler=scheduler)\n",
    "print(len(images))\n",
    "plt.figure()\n",
    "plt.imshow(images[0, 0].cpu(), vmin=0, vmax=1, cmap=\"bone\")\n",
    "plt.colorbar()\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "for i in range(len(images)):\n",
    "    print(images[i, 0].cpu().shape, images[i, 0].cpu().type)\n",
    "    numpy_arr = images[i, 0].detach().cpu().numpy()\n",
    "    #numpy_arr_scaled = 255 * numpy_arr\n",
    "    plt.imshow(numpy_arr, vmin = 0, vmax = 1, cmap = \"bone\")\n",
    "    #plt.imshow(numpy_arr_scaled, cmap = \"bone\")\n",
    "    #cv2.imwrite(\"Genererte_\" + str(i) + \".png\", numpy_arr_scaled)\n",
    "    nifti_image = nib.Nifti1Image(numpy_arr,np.eye(4))\n",
    "    #nib.save(nifti_image, \"Synthetic_images/bs16_125epochs_3nov/nifti_file_\" + str(i) + \".nii\") #125\n",
    "    #nib.save(nifti_image, \"Synthetic_images/bs8_150epochs_8nov/nifti_file_\" + str(i) + \".nii\")\n",
    "    nib.save(nifti_image, \"Synthetic_images/bs16_150epochs_22_nov_larger_dataset/nifti_file_\" + str(i+400) + \".nii\")\n",
    "    #nib.save(nifti_image, \"Synthetic_images/bs16_150epochs_timestep500/nifti_file_\" + str(i) + \".nii\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bee413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "f_transform = np.fft.fft2(numpy_arr_scaled)\n",
    "print(f_transform.shape)\n",
    "f_transform_shifted = np.fft.fftshift(f_transform)\n",
    "power_spectrum = np.abs(f_transform_shifted) ** 2\n",
    "plt.imshow(np.log1p(power_spectrum), cmap='gray')\n",
    "plt.title('Fourier transform of synthetic image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088a826d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(numpy_arr_scaled.flatten(), bins=256)\n",
    "plt.title(\"Histogram for synthetic image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8715b786",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(images[1, 0].cpu(), vmin=0, vmax=1, cmap=\"bone\")\n",
    "plt.figure()\n",
    "plt.imshow(images[2, 0].cpu(), vmin=0, vmax=1, cmap=\"bone\")\n",
    "plt.figure()\n",
    "#plt.imshow(images[3, 0].cpu(), vmin=0, vmax=1, cmap=\"bone\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f752be",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, real_images = random_split(val, [len(val) - 4, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450d5768",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef14f31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "radnet = torch.hub.load(\"Warvito/radimagenet-models\", model=\"radimagenet_resnet50\", verbose=True)\n",
    "radnet.to(device)\n",
    "radnet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e64ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((Dataset(real_images).__getitem__(0).shape))\n",
    "print(real_images.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37064d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(images[0].shape)\n",
    "image = np.repeat(images[0], 3, axis=0)\n",
    "print(image.shape)\n",
    "scaled_image = image*255\n",
    "print(scaled_image.dtype)\n",
    "#int_img = scaled_image.type(torch.uint8)\n",
    "#print(int_img.dtype)\n",
    "batched_image = torch.unsqueeze(scaled_image, axis=0)\n",
    "#batched_image = batched_image.type(torch.uint8)\n",
    "print(\"final shape\", batched_image.shape)\n",
    "print(batched_image.dtype)\n",
    "\n",
    "real_image = images[1]\n",
    "print(real_image.shape)\n",
    "print(real_image.type)\n",
    "import numpy as np\n",
    "a = np.repeat(real_image, 3, axis=0)\n",
    "print(image.shape)\n",
    "b = a*255\n",
    "#print(b.dtype)\n",
    "#c = b.type(torch.uint8)\n",
    "#print(c.dtype)\n",
    "d = torch.unsqueeze(b, axis=0)\n",
    "#d = d.type(torch.uint8)\n",
    "print(d.shape)\n",
    "print(d.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b670d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''The function calculate_FID assumes that the input is a RGB image on the form\n",
    "[]\n",
    "\n",
    "'''\n",
    "fid = calculate_FID(batched_image, d, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c82fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "# Define functions to calculate mean and covariance of feature embeddings\n",
    "\n",
    "def calculate_fid(model, real_images, generated_images):\n",
    "    real_feature_activations = model(real_images).detach().cpu().numpy()\n",
    "    generated_feature_activations = model(generated_images).detach().cpu().numpy()\n",
    "\n",
    "    # Calculate mean and covariance for real and generated feature embeddings\n",
    "    mu_real = np.mean(real_feature_activations, axis=0)\n",
    "    mu_generated = np.mean(generated_feature_activations, axis=0)\n",
    "    cov_real = np.cov(real_feature_activations, rowvar=False)\n",
    "    cov_generated = np.cov(generated_feature_activations, rowvar=False)\n",
    "\n",
    "    # Calculate the FID score\n",
    "    cov_sqrt = sqrtm(cov_real.dot(cov_generated))\n",
    "    if np.iscomplexobj(cov_sqrt):\n",
    "        cov_sqrt = cov_sqrt.real\n",
    "    fid = np.sum((mu_real - mu_generated) ** 2) + np.trace(cov_real + cov_generated - 2 * cov_sqrt)\n",
    "\n",
    "    return fid\n",
    "\n",
    "# Load a pre-trained InceptionV3 model\n",
    "inception_model = torch.hub.load('pytorch/vision', 'inception_v3', pretrained=True)\n",
    "\n",
    "# Calculate FID score\n",
    "fid_score = calculate_fid(inception_model, real_images, images)\n",
    "print(f'FID Score: {fid_score}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29941dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "image = np.repeat(images[0], 3, axis=0)\n",
    "scaled_image = image*255\n",
    "\n",
    "import torch\n",
    "_ = torch.manual_seed(123)\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance #ÆÆÆÆÆ funker ikke på greyscale\n",
    "fid = FrechetInceptionDistance(feature=64)\n",
    "# generate two slightly overlapping image intensity distributions\n",
    "imgs_dist1 = torch.randint(0, 200, (100, 3, 299, 299), dtype=torch.uint8)\n",
    "imgs_dist2 = torch.randint(100, 255, (100, 3, 299, 299), dtype=torch.uint8)\n",
    "print(imgs_dist2.shape)\n",
    "print(imgs_dist2.type)\n",
    "\n",
    "\n",
    "\n",
    "fid.update(, real=True)\n",
    "fid.update(imgs_dist2, real=False)\n",
    "fid.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afaf600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(images[0].shape)\n",
    "image = np.repeat(images[0], 3, axis=0)\n",
    "print(image.shape)\n",
    "scaled_image = image*255\n",
    "print(scaled_image.dtype)\n",
    "int_img = scaled_image.type(torch.uint8)\n",
    "print(int_img.dtype)\n",
    "batched_image = torch.unsqueeze(int_img, axis=0)\n",
    "batched_image = batched_image.type(torch.uint8)\n",
    "print(\"final shape\", batched_image.shape)\n",
    "print(batched_image.dtype)\n",
    "\n",
    "real_image = images[1]\n",
    "print(real_image.shape)\n",
    "print(real_image.type)\n",
    "import numpy as np\n",
    "a = np.repeat(real_image, 3, axis=0)\n",
    "print(image.shape)\n",
    "b = a*255\n",
    "print(b.dtype)\n",
    "c = b.type(torch.uint8)\n",
    "print(c.dtype)\n",
    "d = torch.unsqueeze(b, axis=0)\n",
    "d = d.type(torch.uint8)\n",
    "print(d.shape)\n",
    "print(d.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ddba74",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(batched_image[0, 0], vmin = 0, vmax = 255)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9668539a",
   "metadata": {},
   "outputs": [],
   "source": [
    "real = d.numpy()\n",
    "print(real.dtype)\n",
    "real = torch.from_numpy(real)\n",
    "print(real.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f709105",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "_ = torch.manual_seed(123)\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance #ÆÆÆÆÆ funker ikke på greyscale\n",
    "fid = FrechetInceptionDistance(feature=64)\n",
    "\n",
    "fid.update([batched_image, real], real=True)\n",
    "fid.update(real, real=False)\n",
    "fid.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5098172",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f483bee6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
