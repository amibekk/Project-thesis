{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb83534a",
   "metadata": {},
   "source": [
    "# Generating synthetic MRI data using diffusion models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9744b86",
   "metadata": {},
   "source": [
    "This notebook loads and preprocesses T2 weighted MR images of patients with rectal cancer. Preprocessing consists of normalizing and downsampling the images. Cropping is also possible. Then a diffusion model is defined and subsequently trained. The following tutorial from the MONAI-generative framework have been used when defining and training the diffusion model:  \n",
    "https://github.com/Project-MONAI/GenerativeModels/blob/main/tutorials/generative/2d_ddpm/2d_ddpm_tutorial.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f56cf9",
   "metadata": {},
   "source": [
    "### Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4968b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from monai.transforms import Compose, LoadImage, ToTensor, ScaleIntensity, CenterSpatialCrop, Resize, EnsureChannelFirst, RandAffined, SaveImage,Rotate90\n",
    "from monai.apps import MedNISTDataset\n",
    "from monai.config import print_config\n",
    "from monai.data import CacheDataset, DataLoader, Dataset, ArrayDataset\n",
    "from monai.utils import first, set_determinism\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import ConcatDataset,random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from generative.inferers import DiffusionInferer\n",
    "from generative.networks.nets import DiffusionModelUNet\n",
    "from generative.networks.schedulers import DDPMScheduler\n",
    "import monai\n",
    "import cv2\n",
    "from datetime import datetime\n",
    "#from PIL import Image\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1971d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf6c9ea",
   "metadata": {},
   "source": [
    "Checking GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd877973",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbd28b7",
   "metadata": {},
   "source": [
    "### Loading and preprocessing MRI dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef6fbfc",
   "metadata": {},
   "source": [
    "PREPROCESSING: Scaling the intensity to interval [0, 1]. Cropping the image and resizing it. Image should now be of size 128 x 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4949fdd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_transform = Compose(\n",
    "    [LoadImage(image_only = True),\n",
    "     EnsureChannelFirst(),\n",
    "     ToTensor(),\n",
    "     #ScaleIntensity(minv = 0.0, maxv = 1.0),\n",
    "     #CenterSpatialCrop(roi_size=(256,256,-1)),\n",
    "     Resize(spatial_size=(128, 128, -1))\n",
    "     #Resize(spatial_size=(64, 64, -1))\n",
    "     ])\n",
    "\n",
    "image_transforms = Compose(\n",
    "    [\n",
    "     ScaleIntensity(minv = 0.0, maxv = 1.0),\n",
    "     Rotate90(k=3, spatial_axes=(0, 1), lazy=False),\n",
    "     ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4d40e6",
   "metadata": {},
   "source": [
    "Creating NiFTIDataset-class that inherits from monai.Dataset. Each dataelement is a niftifile. If transform is applied, only the images are loaded (any additional info removed). \n",
    "Hence, each dataelement a stack of images. The function extract_slices collects the image slices and creates a new dataset only consisting of these 2D images. All sluces, except for edge slices, are kept for further training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ffb88f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class NiFTIDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform = None):\n",
    "        self.data_dir = data_dir\n",
    "        self.data = os.listdir(data_dir)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        nifti_file = os.path.join(self.data_dir, self.data[index])\n",
    "        if self.transform is not None:\n",
    "            nifti_file = self.transform(nifti_file)\n",
    "        return nifti_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6883400b",
   "metadata": {},
   "source": [
    "### Loading more images into the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82873e52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''Function that extracts 2D slices and creates one large dataset consisting of these '''\n",
    "def extract_slices(nifti_dataset): \n",
    "    total_dataset = Dataset([])\n",
    "    for i in range(len(nifti_dataset)):\n",
    "        image_stack = Dataset(nifti_dataset).__getitem__(index = i)\n",
    "        for j in range(image_stack.shape[3]):\n",
    "            image_stack[:,:,:,j] = image_transforms(image_stack[:,:,:,j])\n",
    "        images = Dataset([image_stack[:,:,:,k] for k in range(3, image_stack.shape[3] - 3)])\n",
    "        total_dataset = ConcatDataset([total_dataset, images])\n",
    "    \n",
    "    return total_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5ebcbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nifti_dataset = NiFTIDataset(data_dir= \"T2_images\", transform = image_transform)\n",
    "#nifti_dataset_org = NiFTIDataset(data_dir= \"T2_images\")\n",
    "data = extract_slices(nifti_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79fda4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nifti_dataset_org = NiFTIDataset(data_dir= \"T2_images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f573b5e9",
   "metadata": {},
   "source": [
    "### Prelimenary visualizing of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebcb04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure()\n",
    "plt.imshow(data.__getitem__(0)[0,:,:],cmap = \"gray\")\n",
    "plt.colorbar()\n",
    "#plt.title(\"Preprocessed image\")\n",
    "plt.show()\n",
    "\n",
    "org_file = nib.load(nifti_dataset_org.__getitem__(index = 0))\n",
    "print(org_file.shape)\n",
    "slice_1 = org_file.get_fdata()[:,:,3]\n",
    "print(slice_1.shape)\n",
    "slice_1 = np.rot90(slice_1, k=3)\n",
    "\n",
    "#print(\"diminfo:\", slice_1['dim_info'])\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(slice_1, cmap = \"gray\")\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "for i in range(109):\n",
    "    org_file = nib.load(nifti_dataset_org.__getitem__(index = i))\n",
    "    print(i, org_file.shape)\n",
    "    print(org_file)\n",
    "    \n",
    "    org_file = nib.load(nifti_dataset_org.__getitem__(index = i))\n",
    "    print(org_file.shape)\n",
    "    slice_1 = org_file.get_fdata()[:,:,3]\n",
    "    print(slice_1.shape)\n",
    "    slice_1 = np.rot90(slice_1, k=3)\n",
    "\n",
    "#print(\"diminfo:\", slice_1['dim_info'])\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(slice_1, cmap = \"gray\")\n",
    "    plt.colorbar()\n",
    "    plt.title(str(i))\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a6fdd4",
   "metadata": {},
   "source": [
    "### Creating validation and training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81956a06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''Hold out validation: Need to hold back some patients for validation purposes, need to work with the nifti_dataset'''\n",
    "train_ratio = 0.8 #Possible to choose another split-ratio\n",
    "\n",
    "train_patiens = int(train_ratio * len(nifti_dataset))\n",
    "val_patiens = len(nifti_dataset) - train_patiens\n",
    "\n",
    "train_nifti_dataset, val_nifti_dataset = random_split(nifti_dataset, [train_patiens, val_patiens])\n",
    "\n",
    "'''train_dataset and val_dataset needs to be fed into extract_slices. ! check input parameter !'''\n",
    "\n",
    "print(len(train_nifti_dataset), len(val_nifti_dataset))\n",
    "\n",
    "train_dataset = extract_slices(train_nifti_dataset)\n",
    "val_dataset = extract_slices(val_nifti_dataset)\n",
    "\n",
    "print(len(train_dataset), len(val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23081296",
   "metadata": {},
   "source": [
    "### Visualizing training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89753aa9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(train_dataset)):\n",
    "    if (i % 100 == 0):\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.imshow(Dataset(train_dataset).__getitem__(index = i)[0,:,:],cmap = \"bone\")\n",
    "        plt.colorbar()\n",
    "        plt.title(\"Dataelement \" + str(i))\n",
    "        plt.show()\n",
    "        \n",
    "j = 0        \n",
    "for real_image in train_dataset:\n",
    "   # print(torch.amax(real_image[0]))\n",
    "    real_image = real_image.numpy()\n",
    "   # print(np.amax(real_image[0]))\n",
    "    nifti_image = nib.Nifti1Image(real_image[0],np.eye(4))\n",
    "    #nib.save(nifti_image, \"Real_images/Real_training_data_bs8_8nov/nifti_file_\" + str(j) + \".nii\")\n",
    "    j+=1\n",
    "'''\n",
    "k = 0    \n",
    "for val_image in val_dataset:\n",
    "   # print(torch.amax(val_image[0]))\n",
    "    val_image = val_image.numpy()\n",
    "   # print(np.amax(val_image[0]))\n",
    "    nifti_image = nib.Nifti1Image(val_image[0],np.eye(4))\n",
    "    nib.save(nifti_image, \"Real_images/Real_validation_data_bs8_8nov/nifti_file_\" + str(k) + \".nii\")\n",
    "    k+=1'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c65df1",
   "metadata": {},
   "source": [
    "### Visualizing and saving validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe9f2a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(val_dataset)):\n",
    "    if (i % 100 == 0):\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.imshow(Dataset(val_dataset).__getitem__(index = i)[0,:,:],cmap = \"bone\")\n",
    "        plt.colorbar()\n",
    "        plt.title(\"Dataelement \" + str(i))\n",
    "        plt.show()\n",
    "j = 0        \n",
    "for val_image in val_dataset:\n",
    "   # print(torch.amax(val_image[0]))\n",
    "    val_image = val_image.numpy()\n",
    "   # print(np.amax(val_image[0]))\n",
    "    nifti_image = nib.Nifti1Image(val_image[0],np.eye(4))\n",
    "    nib.save(nifti_image, \"Real_images/Real_validation_data_bs16_for_epoch250/nifti_file_\" + str(j) + \".nii\")\n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a488536",
   "metadata": {},
   "source": [
    "### Loading dataset into dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6221676d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bs = 16\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True, num_workers = 4, persistent_workers = True)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=bs, shuffle=True, num_workers = 4, persistent_workers = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1eb07b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "set_determinism(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbd3fc0",
   "metadata": {},
   "source": [
    "### Defining diffusion model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b9e5e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model = DiffusionModelUNet(\n",
    "    spatial_dims=2,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    num_channels=(128, 256, 256), #256, 256, 512\n",
    "    attention_levels=(False, True, True),\n",
    "    num_res_blocks=1,\n",
    "    num_head_channels=256,\n",
    ")\n",
    "\n",
    "#Loading pre-trained model with the same architecture as above. This model is however trained on the MedNIST Hand dataset.\n",
    "pre_trained_model = torch.hub.load(\"marksgraham/pretrained_generative_models:v0.2\", model=\"ddpm_2d\", verbose=True) \n",
    "state_dict = pre_trained_model.state_dict()\n",
    "model.load_state_dict(state_dict, strict = False) \n",
    "\n",
    "model.to(device)\n",
    "\n",
    "scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=2.5e-5)\n",
    "\n",
    "inferer = DiffusionInferer(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a07781",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''Saving the images during training'''\n",
    "saver = SaveImage(\n",
    "    output_dir=\"Generated_images\",\n",
    "    output_ext=\".png\",\n",
    "    output_postfix=\"itk\",\n",
    "    output_dtype=np.uint8,\n",
    "    resample=False,\n",
    "    writer=\"ITKWriter\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd22610",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b228c9b3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_epochs = 200\n",
    "val_interval = 25\n",
    "'''Lists for metrics in order to tune hyperparameters'''\n",
    "epoch_accuracy_list = []\n",
    "val_epoch_accuracy_list = []\n",
    "epoch_loss_list = []\n",
    "val_epoch_loss_list = []\n",
    "epoch_f1_score_list = []\n",
    "val_epoch_f1_score_list = []\n",
    "epoch_precision_list = []\n",
    "val_epoch_precision_list = []\n",
    "epoch_recall_list = []\n",
    "val_epoch_recall_list = []\n",
    "\n",
    "scaler = GradScaler()\n",
    "total_start = time.time()\n",
    "\n",
    "'''For tensorboard'''\n",
    "writer = SummaryWriter()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    #Epoch metrics\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "    epoch_f1_score = 0\n",
    "    epoch_precision = 0\n",
    "    epoch_recall = 0\n",
    "    \n",
    "    progress_bar = tqdm(enumerate(train_data_loader), total=len(train_data_loader), ncols=70)\n",
    "    progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "    for step, batch in progress_bar: \n",
    "        images = batch.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with autocast(enabled=True):\n",
    "            '''Generate noise with equal shape as images in order overlay the noise on these. Create timesteps and get noise prediction'''\n",
    "            noise = torch.randn_like(images).to(device)\n",
    "            timesteps = torch.randint(0, inferer.scheduler.num_train_timesteps, (images.shape[0],), device=images.device).long()\n",
    "            noise_pred = inferer(inputs=images, diffusion_model=model, noise=noise, timesteps=timesteps)\n",
    "            '''Compare the predicted noise to the actual noise'''\n",
    "            loss = F.mse_loss(noise_pred.float(), noise.float()) \n",
    "            \n",
    "            conf_matrix = monai.metrics.get_confusion_matrix(noise_pred.float(),noise.float())\n",
    "            accuracy = monai.metrics.compute_confusion_matrix_metric(\"accuracy\", conf_matrix)\n",
    "            f1_score = monai.metrics.compute_confusion_matrix_metric(\"f1 score\", conf_matrix)\n",
    "            precision = monai.metrics.compute_confusion_matrix_metric(\"precision\", conf_matrix)\n",
    "            recall = monai.metrics.compute_confusion_matrix_metric(\"recall\", conf_matrix)\n",
    "            \n",
    "        #writer.add_scalar(\"Loss/train\", loss, epoch)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        epoch_accuracy += torch.mean(accuracy).cpu()\n",
    "        epoch_f1_score += torch.mean(f1_score).cpu()\n",
    "        epoch_precision += torch.mean(precision).cpu()\n",
    "        epoch_recall += torch.mean(recall).cpu()\n",
    "        epoch_loss += loss.item() \n",
    "        #print(\"Accuracy: \", epoch_accuracy / (step + 1), \",precision: \", epoch_precision /(step + 1), \",Loss: \", epoch_loss /(step + 1))\n",
    "\n",
    "        progress_bar.set_postfix({\"loss\": epoch_loss / (step + 1)})\n",
    "    epoch_loss_list.append(epoch_loss / (step + 1))\n",
    "    writer.add_scalar(\"Loss/train\", epoch_loss /(step + 1), epoch)\n",
    "    epoch_accuracy_list.append(epoch_accuracy / (step + 1))\n",
    "    writer.add_scalar(\"Accuracy/train\", epoch_accuracy /(step + 1), epoch)\n",
    "    epoch_f1_score_list.append(epoch_f1_score / (step + 1))\n",
    "    writer.add_scalar(\"F1_score/train\", epoch_f1_score /(step + 1), epoch)\n",
    "    epoch_precision_list.append(epoch_precision / (step + 1))\n",
    "    writer.add_scalar(\"Precision/train\", epoch_precision /(step + 1), epoch)\n",
    "    epoch_recall_list.append(epoch_recall / (step + 1))\n",
    "    writer.add_scalar(\"Recall/train\", epoch_recall /(step + 1), epoch)\n",
    "    \n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval()\n",
    "        '''Saving model'''\n",
    "        path = \"Models/bs\" + str(bs) + \"_Epoch\" + str(epoch) + \"_of_\" + str(n_epochs) + \"8nov\"\n",
    "        torch.save(model.state_dict(), path)\n",
    "        #Validation epoch metrics\n",
    "        val_epoch_loss = 0\n",
    "        val_epoch_accuracy = 0\n",
    "        val_epoch_f1_score = 0\n",
    "        val_epoch_precision = 0\n",
    "        val_epoch_recall = 0\n",
    "        \n",
    "        for step, batch in enumerate(val_data_loader):\n",
    "            images = batch.to(device)\n",
    "            with torch.no_grad():\n",
    "                with autocast(enabled=True):\n",
    "                    noise = torch.randn_like(images).to(device)\n",
    "                    timesteps = torch.randint(0, inferer.scheduler.num_train_timesteps, (images.shape[0],), device=images.device).long()\n",
    "                    noise_pred = inferer(inputs=images, diffusion_model=model, noise=noise, timesteps=timesteps)\n",
    "                    val_loss = F.mse_loss(noise_pred.float(), noise.float())\n",
    "                    \n",
    "                    val_conf_matrix = monai.metrics.get_confusion_matrix(noise_pred.float(),noise.float())\n",
    "                    val_accuracy = monai.metrics.compute_confusion_matrix_metric(\"accuracy\", val_conf_matrix)\n",
    "                    val_f1_score = monai.metrics.compute_confusion_matrix_metric(\"f1 score\", val_conf_matrix)\n",
    "                    val_precision = monai.metrics.compute_confusion_matrix_metric(\"precision\", val_conf_matrix)\n",
    "                    val_recall = monai.metrics.compute_confusion_matrix_metric(\"recall\", val_conf_matrix)\n",
    "\n",
    "            val_epoch_loss += val_loss.item()\n",
    "            val_epoch_accuracy = torch.mean(val_accuracy).cpu()\n",
    "            val_epoch_f1_score = torch.mean(val_f1_score).cpu()\n",
    "            val_epoch_precision = torch.mean(val_precision).cpu()\n",
    "            val_epoch_recall= torch.mean(val_recall).cpu()\n",
    "            progress_bar.set_postfix({\"val_loss\": val_epoch_loss / (step + 1)})\n",
    "        \n",
    "        val_epoch_loss_list.append(val_epoch_loss / (step + 1))\n",
    "        val_epoch_accuracy_list.append(val_epoch_loss / (step + 1))\n",
    "        val_epoch_f1_score_list.append(val_epoch_f1_score / (step + 1))\n",
    "        val_epoch_precision_list.append(val_epoch_precision / (step + 1))\n",
    "        val_epoch_recall_list.append(val_epoch_recall / (step + 1))\n",
    "        \n",
    "        writer.add_scalar(\"Loss/val\", val_epoch_loss /(step + 1), epoch)\n",
    "        writer.add_scalar(\"Accuracy/val\", val_epoch_accuracy/(step + 1), epoch)\n",
    "        writer.add_scalar(\"F1_score/val\", val_epoch_f1_score/(step + 1), epoch)\n",
    "        writer.add_scalar(\"Precision/val\", val_epoch_precision/(step + 1), epoch)\n",
    "        writer.add_scalar(\"Recall/val\", val_epoch_recall/(step + 1), epoch)\n",
    "\n",
    "\n",
    "        '''Sampling images from random noise to visualize during training'''\n",
    "        noise = torch.randn((1, 1, 128, 128))\n",
    "        noise = noise.to(device)\n",
    "        scheduler.set_timesteps(num_inference_steps=1000)\n",
    "        with autocast(enabled=True):\n",
    "            image = inferer.sample(input_noise=noise, diffusion_model=model, scheduler=scheduler)\n",
    "\n",
    "        plt.figure()\n",
    "        plt.imshow(image[0, 0].cpu(), cmap=\"gray\")\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "\n",
    "total_time = time.time() - total_start\n",
    "print(f\"train completed, total time: {total_time}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941ce574",
   "metadata": {},
   "source": [
    "### Training progress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349f0c0f",
   "metadata": {},
   "source": [
    "Plotting the loss function over the number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89449e70",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.figure()\n",
    "plt.plot(np.linspace(1, n_epochs, n_epochs), epoch_loss_list, color=\"red\", linewidth=2.0, label=\"Train\")\n",
    "plt.plot(np.linspace(1, n_epochs, int(n_epochs / val_interval)), val_epoch_loss_list, \"go-\",linewidth=2.0, label=\"Validation\")\n",
    "plt.xlabel(\"Epochs\", fontsize=16)\n",
    "plt.ylabel(\"Loss\", fontsize=16)\n",
    "plt.legend(prop={\"size\": 14})\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.linspace(1, n_epochs, n_epochs), epoch_recall_list, color=\"red\", linewidth=2.0, label=\"Train\")\n",
    "plt.plot(np.linspace(1, n_epochs, int(n_epochs / val_interval)), val_epoch_recall_list, \"go-\",linewidth=2.0, label=\"Validation\")\n",
    "plt.xlabel(\"Epochs\", fontsize=16)\n",
    "plt.ylabel(\"Loss\", fontsize=16)\n",
    "plt.legend(prop={\"size\": 14})\n",
    "plt.show()\n",
    "\n",
    "\n",
    "loss = np.array(epoch_loss_list)\n",
    "print(\"Training loss:\", np.around(loss, 3))\n",
    "\n",
    "val_loss = np.array(val_epoch_loss_list)\n",
    "print(\"Validation loss:\", np.around(val_loss, 3))\n",
    "\n",
    "recall_train = np.array(epoch_recall_list)\n",
    "print(\"Validation loss:\", np.around(recall_train, 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
